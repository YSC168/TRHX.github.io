---
title: Python3 çˆ¬è™«å®æˆ˜ â€” å®‰å±…å®¢æ­¦æ±‰äºŒæ‰‹æˆ¿
tags:
  - çˆ¬è™«
  - å®‰å±…å®¢
categories: 
  - Python3 å­¦ä¹ ç¬”è®°
  - çˆ¬è™«å®æˆ˜
thumbnail: https://cdn.jsdelivr.net/gh/TRHX/ImageHosting/ITRHX-PIC/thumbnail/combat.png
avatar: https://cdn.jsdelivr.net/gh/TRHX/CDN-for-itrhx.com@2.1.9/images/trhx.png
---

> çˆ¬å–æ—¶é—´ï¼š2019-10-09
> çˆ¬å–éš¾åº¦ï¼šâ˜…â˜…â˜†â˜†â˜†â˜†
> è¯·æ±‚é“¾æ¥ï¼šhttps://wuhan.anjuke.com/sale/
> çˆ¬å–ç›®æ ‡ï¼šçˆ¬å–æ­¦æ±‰äºŒæ‰‹æˆ¿æ¯ä¸€æ¡å”®æˆ¿ä¿¡æ¯ï¼ŒåŒ…å«åœ°ç†ä½ç½®ã€ä»·æ ¼ã€é¢ç§¯ç­‰ï¼Œä¿å­˜ä¸º CSV æ–‡ä»¶
> æ¶‰åŠçŸ¥è¯†ï¼šè¯·æ±‚åº“ requestsã€è§£æåº“ Beautiful Soupã€CSV æ–‡ä»¶å‚¨å­˜ã€åˆ—è¡¨æ“ä½œã€åˆ†é¡µåˆ¤æ–­
> å®Œæ•´ä»£ç ï¼šhttps://github.com/TRHX/Python3-Spider-Practice/tree/master/anjuke
> å…¶ä»–çˆ¬è™«å®æˆ˜ä»£ç åˆé›†ï¼ˆæŒç»­æ›´æ–°ï¼‰ï¼šhttps://github.com/TRHX/Python3-Spider-Practice
> çˆ¬è™«å®æˆ˜ä¸“æ ï¼ˆæŒç»­æ›´æ–°ï¼‰ï¼šhttps://itrhx.blog.csdn.net/article/category/9351278

---

<!--more-->

# <font color=#FF0000>ã€1x00ã€‘é¡µé¢æ•´ä½“åˆ†æ</font>

åˆ†æ [å®‰å±…å®¢æ­¦æ±‰äºŒæ‰‹æˆ¿é¡µé¢](https://wuhan.anjuke.com/sale/)ï¼Œè¿™æ¬¡çˆ¬å–å®æˆ˜å‡†å¤‡ä½¿ç”¨ BeautifulSoup è§£æåº“ï¼Œç†Ÿç»ƒ BeautifulSoup è§£æåº“çš„ç”¨æ³•ï¼Œæ³¨æ„åˆ°è¯¥é¡µé¢ä¸å…¶ä»–é¡µé¢ä¸åŒçš„æ˜¯ï¼Œä¸èƒ½ä¸€æ¬¡æ€§çœ‹åˆ°åˆ°åº•æœ‰å¤šå°‘é¡µï¼Œä»¥å‰çŸ¥é“ä¸€å…±æœ‰å¤šå°‘é¡µï¼Œç›´æ¥ä¸€ä¸ªå¾ªç¯çˆ¬å–å°±è¡Œäº†ï¼Œè™½ç„¶å¯ä»¥é€šè¿‡æ”¹å˜ url æ¥å°è¯•æ‰¾åˆ°æœ€åä¸€é¡µï¼Œä½†æ˜¯è¿™æ ·å°±æ˜¾å¾—ä¸ç¨‹åºå‘˜äº†ğŸ˜‚ï¼Œå› æ­¤å¯ä»¥é€šè¿‡ BeautifulSoup è§£æ `ä¸‹ä¸€é¡µæŒ‰é’®`ï¼Œæå–åˆ°ä¸‹ä¸€é¡µçš„ urlï¼Œç›´åˆ°æ²¡æœ‰ `ä¸‹ä¸€é¡µæŒ‰é’®` è¿™ä¸ªå…ƒç´ ä¸ºæ­¢ï¼Œä»è€Œå®ç°æ‰€æœ‰é¡µé¢çš„çˆ¬å–ï¼Œå‰©ä¸‹çš„ä¿¡æ¯æå–å’Œå‚¨å­˜å°±æ¯”è¾ƒç®€å•äº†

---

# <font color=#FF0000>ã€2x00ã€‘è§£ææ¨¡å—</font>

åˆ†æé¡µé¢ï¼Œå¯ä»¥å‘ç°æ¯æ¡äºŒæ‰‹æˆ¿ä¿¡æ¯éƒ½æ˜¯åŒ…å«åœ¨ `<li> ` æ ‡ç­¾å†…çš„ï¼Œå› æ­¤å¯ä»¥ä½¿ç”¨ BeautifulSoup è§£æé¡µé¢å¾—åˆ°æ‰€æœ‰çš„ `<li> ` æ ‡ç­¾ï¼Œç„¶åå†å¾ªç¯è®¿é—®æ¯ä¸ª `<li>` æ ‡ç­¾ï¼Œä¾æ¬¡è§£æå¾—åˆ°æ¯æ¡äºŒæ‰‹æˆ¿çš„å„ç§ä¿¡æ¯

<fancybox>
![01](https://cdn.jsdelivr.net/gh/TRHX/ImageHosting/ITRHX-PIC/A54/01.png)
</fancybox>

```python
def parse_pages(url, num):
    response = requests.get(url=url, headers=headers)
    soup = BeautifulSoup(response.text, 'lxml')
    result_list = soup.find_all('li', class_='list-item')
    # print(len(result_list))
    for result in result_list:
        # æ ‡é¢˜
        title = result.find('a', class_='houseListTitle').text.strip()
        # print(title)
        # æˆ·å‹
        layout = result.select('.details-item > span')[0].text
        # print(layout)
        # é¢ç§¯
        cover = result.select('.details-item > span')[1].text
        # print(cover)
        # æ¥¼å±‚
        floor = result.select('.details-item > span')[2].text
        # print(floor)
        # å»ºé€ å¹´ä»½
        year = result.select('.details-item > span')[3].text
        # print(year)
        # å•ä»·
        unit_price = result.find('span', class_='unit-price').text.strip()
        # print(unit_price)
        # æ€»ä»·
        total_price = result.find('span', class_='price-det').text.strip()
        # print(total_price)
        # å…³é”®å­—
        keyword = result.find('div', class_='tags-bottom').text.strip()
        # print(keyword)
        # åœ°å€
        address = result.find('span', class_='comm-address').text.replace(' ', '').replace('\n', '')
        # print(address)
        # è¯¦æƒ…é¡µurl
        details_url = result.find('a', class_='houseListTitle')['href']
        # print(details_url)

if __name__ == '__main__':
    start_num = 0
    start_url = 'https://wuhan.anjuke.com/sale/'
    parse_pages(start_url, start_num)

```

---

# <font color=#FF0000>ã€3x00ã€‘å¾ªç¯çˆ¬å–æ¨¡å—</font>

å‰é¢å·²ç»åˆ†æè¿‡ï¼Œè¯¥ç½‘é¡µæ˜¯æ— æ³•ä¸€ä¸‹å°±èƒ½çœ‹åˆ°ä¸€å…±æœ‰å¤šå°‘é¡µçš„ï¼Œå°è¯•æ‰¾åˆ°æœ€åä¸€é¡µï¼Œå‘ç°ä¸€å…±æœ‰50é¡µï¼Œé‚£ä¹ˆæ­¤æ—¶å°±å¯ä»¥æä¸ªå¾ªç¯ï¼Œä¸€ç›´åˆ°ç¬¬50é¡µå°±è¡Œäº†ï¼Œä½†æ˜¯å¦‚æœæœ‰ä¸€å¤©é¡µé¢æ•°å¢åŠ äº†å‘¢ï¼Œé‚£ä¹ˆä»£ç çš„å¯ç»´æŠ¤æ€§å°±ä¸å¥½äº†ï¼Œæˆ‘ä»¬å¯ä»¥è§‚å¯Ÿ `ä¸‹ä¸€é¡µæŒ‰é’®` ï¼Œå½“å­˜åœ¨ä¸‹ä¸€é¡µçš„æ—¶å€™ï¼Œæ˜¯ `<a>` æ ‡ç­¾ï¼Œå¹¶ä¸”å¸¦æœ‰ä¸‹ä¸€é¡µçš„ URLï¼Œä¸å­˜åœ¨ä¸‹ä¸€é¡µçš„æ—¶å€™æ˜¯ `<i>` æ ‡ç­¾ï¼Œå› æ­¤å¯ä»¥å†™ä¸ª `if` è¯­å¥ï¼Œåˆ¤æ–­æ˜¯å¦å­˜åœ¨æ­¤ `<a>` æ ‡ç­¾ï¼Œè‹¥å­˜åœ¨ï¼Œè¡¨ç¤ºæœ‰ä¸‹ä¸€é¡µï¼Œç„¶åæå–å…¶ `href` å±æ€§å¹¶ä¼ ç»™è§£ææ¨¡å—ï¼Œå®ç°åé¢æ‰€æœ‰é¡µé¢çš„ä¿¡æ¯æå–ï¼Œæ­¤å¤–ï¼Œç”±äºå®‰å±…å®¢æœ‰åçˆ¬ç³»ç»Ÿï¼Œæˆ‘ä»¬è¿˜å¯ä»¥åˆ©ç”¨ Pythonä¸­çš„ `random.randint()` æ–¹æ³•ï¼Œåœ¨ä¸¤ä¸ªæ•°å€¼ä¹‹é—´éšæœºå–ä¸€ä¸ªæ•°ï¼Œä¼ å…¥ `time.sleep()` æ–¹æ³•ï¼Œå®ç°éšæœºæš‚åœçˆ¬å–

<fancybox>
![02](https://cdn.jsdelivr.net/gh/TRHX/ImageHosting/ITRHX-PIC/A54/02.png)
</fancybox>

```python
# åˆ¤æ–­æ˜¯å¦è¿˜æœ‰ä¸‹ä¸€é¡µ
next_url = soup.find_all('a', class_='aNxt')
if len(next_url) != 0:
    num += 1
    print('ç¬¬' + str(num) + 'é¡µæ•°æ®çˆ¬å–å®Œæ¯•ï¼')
    # 3-60ç§’ä¹‹é—´éšæœºæš‚åœ
    time.sleep(random.randint(3, 60))
    parse_pages(next_url[0].attrs['href'], num)
else:
    print('æ‰€æœ‰æ•°æ®çˆ¬å–å®Œæ¯•ï¼')

```

---

# <font color=#FF0000>ã€4x00ã€‘æ•°æ®å‚¨å­˜æ¨¡å—</font>

æ•°æ®å‚¨å­˜æ¯”è¾ƒç®€å•ï¼Œå°†æ¯ä¸ªäºŒæ‰‹æˆ¿ä¿¡æ¯ç»„æˆä¸€ä¸ªåˆ—è¡¨ï¼Œä¾æ¬¡å†™å…¥åˆ° anjuke.csv æ–‡ä»¶ä¸­å³å¯

```python
results = [title, layout, cover, floor, year, unit_price, total_price, keyword, address, details_url]
with open('anjuke.csv', 'a', newline='', encoding='utf-8-sig') as f:
    w = csv.writer(f)
    w.writerow(results)

```

---

# <font color=#FF0000>ã€5x00ã€‘å®Œæ•´ä»£ç </font>

```python
# =============================================
# --*-- coding: utf-8 --*--
# @Time    : 2019-10-09
# @Author  : TRHX
# @Blog    : www.itrhx.com
# @CSDN    : https://blog.csdn.net/qq_36759224
# @FileName: anjuke.py
# @Software: PyCharm
# =============================================

import requests
import time
import csv
import random
from bs4 import BeautifulSoup

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.142 Safari/537.36'
}


def parse_pages(url, num):
    response = requests.get(url=url, headers=headers)
    soup = BeautifulSoup(response.text, 'lxml')
    result_list = soup.find_all('li', class_='list-item')
    # print(len(result_list))
    for result in result_list:
        # æ ‡é¢˜
        title = result.find('a', class_='houseListTitle').text.strip()
        # print(title)
        # æˆ·å‹
        layout = result.select('.details-item > span')[0].text
        # print(layout)
        # é¢ç§¯
        cover = result.select('.details-item > span')[1].text
        # print(cover)
        # æ¥¼å±‚
        floor = result.select('.details-item > span')[2].text
        # print(floor)
        # å»ºé€ å¹´ä»½
        year = result.select('.details-item > span')[3].text
        # print(year)
        # å•ä»·
        unit_price = result.find('span', class_='unit-price').text.strip()
        # print(unit_price)
        # æ€»ä»·
        total_price = result.find('span', class_='price-det').text.strip()
        # print(total_price)
        # å…³é”®å­—
        keyword = result.find('div', class_='tags-bottom').text.strip()
        # print(keyword)
        # åœ°å€
        address = result.find('span', class_='comm-address').text.replace(' ', '').replace('\n', '')
        # print(address)
        # è¯¦æƒ…é¡µurl
        details_url = result.find('a', class_='houseListTitle')['href']
        # print(details_url)
        results = [title, layout, cover, floor, year, unit_price, total_price, keyword, address, details_url]
        with open('anjuke.csv', 'a', newline='', encoding='utf-8-sig') as f:
            w = csv.writer(f)
            w.writerow(results)

    # åˆ¤æ–­æ˜¯å¦è¿˜æœ‰ä¸‹ä¸€é¡µ
    next_url = soup.find_all('a', class_='aNxt')
    if len(next_url) != 0:
        num += 1
        print('ç¬¬' + str(num) + 'é¡µæ•°æ®çˆ¬å–å®Œæ¯•ï¼')
        # 3-60ç§’ä¹‹é—´éšæœºæš‚åœ
        time.sleep(random.randint(3, 60))
        parse_pages(next_url[0].attrs['href'], num)
    else:
        print('æ‰€æœ‰æ•°æ®çˆ¬å–å®Œæ¯•ï¼')


if __name__ == '__main__':
    with open('anjuke.csv', 'a', newline='', encoding='utf-8-sig') as fp:
        writer = csv.writer(fp)
        writer.writerow(['æ ‡é¢˜', 'æˆ·å‹', 'é¢ç§¯', 'æ¥¼å±‚', 'å»ºé€ å¹´ä»½', 'å•ä»·', 'æ€»ä»·', 'å…³é”®å­—', 'åœ°å€', 'è¯¦æƒ…é¡µåœ°å€'])
    start_num = 0
    start_url = 'https://wuhan.anjuke.com/sale/'
    parse_pages(start_url, start_num)

```

---

# <font color=#FF0000>ã€6x00ã€‘æ•°æ®æˆªå›¾</font>

<fancybox>
![03](https://cdn.jsdelivr.net/gh/TRHX/ImageHosting/ITRHX-PIC/A54/03.png)
</fancybox>

---
# <font color=#FF0000>ã€7x00ã€‘ç¨‹åºä¸è¶³çš„åœ°æ–¹</font>

- è™½ç„¶ä½¿ç”¨äº†éšæœºæš‚åœçˆ¬å–çš„æ–¹æ³•ï¼Œä½†æ˜¯åœ¨çˆ¬å–äº†å¤§çº¦ 20 é¡µçš„æ•°æ®åä¾ç„¶ä¼šå‡ºç°éªŒè¯é¡µé¢ï¼Œå¯¼è‡´ç¨‹åºç»ˆæ­¢

- åŸæ¥è®¾æƒ³çš„æ˜¯å¯ä»¥ç”±ç”¨æˆ·æ‰‹åŠ¨è¾“å…¥åŸå¸‚çš„æ‹¼éŸ³æ¥æŸ¥è¯¢ä¸åŒåŸå¸‚çš„ä¿¡æ¯ï¼Œæ–¹æ³•æ˜¯æŠŠç”¨æˆ·è¾“å…¥çš„åŸå¸‚æ‹¼éŸ³å’Œå…¶ä»–å‚æ•°ä¸€èµ·æ„é€ æˆä¸€ä¸ª URLï¼Œç„¶åå¯¹è¯¥ URL å‘é€è¯·æ±‚ï¼Œåˆ¤æ–­è¯·æ±‚è¿”å›çš„ä»£ç ï¼Œå¦‚æœæ˜¯ 200 å°±ä»£è¡¨å¯ä»¥è®¿é—®ï¼Œä¹Ÿå°±æ˜¯ç”¨æˆ·è¾“å…¥çš„åŸå¸‚æ˜¯æ­£ç¡®çš„ï¼Œç„¶è€Œå‘ç°å³ä¾¿æ˜¯è¾“å…¥é”™è¯¯ï¼Œè¯¥ URL ä¾ç„¶å¯ä»¥è®¿é—®ï¼Œåªä¸è¿‡ä¼šè·³è½¬åˆ°ä¸€ä¸ªæ­£ç¡®çš„é¡µé¢ï¼Œæ²¡æœ‰ææ¸…æ¥šæ˜¯ä»€ä¹ˆåŸç†ï¼Œä¹Ÿå°±æ— æ³•å®ç°ç”±ç”¨æˆ·è¾“å…¥åŸå¸‚æ¥æŸ¥è¯¢è¿™ä¸ªåŠŸèƒ½
